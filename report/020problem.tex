\section{Problem}
\begin{tcolorbox}
\textbf{Given} a model $\M$, \\
\textbf{Train} $\M$ on a BGP anomaly dataset $\D$ \\
\textbf{Detect} BGP anomalies in other dataset(s)
\end{tcolorbox}

As with any machine learning literature, the current literature for BGP anomaly detection does test for generalization, but only between training and testing sets \textit{drawn from the same anomaly}. While these sorts of results are indicative of good performance for the tested anomalies, they cannot tell us much about the ability of models to generalize across multiple anomalies.

In this project, we present a novel analysis of BGP anomaly detection methods \textit{between} different anomalies. We compare the generalization performance of several anomaly detection approaches, some of which have never been applied to this domain before. We measure the generalizability of these models using accuracy and F1 scores on test sets consisting of multiple anomalies. Furthermore, we explore the features responsible for generalization performance and attempt to interpret their importance.
% Future anomalous events may appear radically different from the events used to piece together previous models, meaning that models trained using supervised methods can offer little guarantee of detecting future events, no matter how accurate they may be at detecting the labeled events. 

% This is essentially an issue of generalizability between different anomalous events, which can be approached using a number of different methods. For this project, we will apply two general approaches commonly used in anomaly detection: one-class learning and unsupervised methods. 
% \begin{itemize}
%     \item \textbf{Regularized classification} - We will attempt to learn a model on one set of anomalous event data and apply well-documented regularization methods to generalize the model to one or more sets of unseen anomalous data.
%     \item \textbf{One-class learning} - The model will be trained on normal BGP traffic data and we will attempt to use it to discriminate between normal and anomalous events.
% \end{itemize}
% \begin{tabbing}
% \textbf{Input:} \hspace{4.2em} \= Time series of BGP features\\
% \textbf{Desired Output:} \> Low generalization error between training and testing events\\
% \textbf{Challenges:} \> Datasets representative of (differences between) events, balancing ability to fit \\
% \> and ability to generalize, dataset imbalance (between events)
% \end{tabbing}

\subsection{Scope}

An "anomaly" in networking can be difficult to define, as it is difficult to precisely characterize the appearance of "normal" traffic. A 2017 survey of BGP anomaly detection approaches by Al-Musawi et al. \cite{al2016bgp} constructs a taxonomy based on the cause of the anomalous behavior. E.g. "Direct" anomalies are caused by problems with the network itself, such as prefix hijacks ("direct intended") or origin misconfigurations("direct unintended"), while indirect anomalies occur as a result of events such as a worm spreading across the Web. 

Finding data for many different anomalies was difficult itself, but compounding that problem was the fact that datasets generally share very few of the same features, which makes comparisons between them difficult. As a result, we limit our dataset to three indirect anomalies caused by computer worms in the early 2000s: Code Red I, Slammer, and Nimda. It would be ideal to include different types of anomalies from different time periods, but we cannot both gather the necessary data and perform the analysis with the time we have. The relative similarity of these anomalies can be beneficial though, as any deficiencies in generalization will represent a sort of upper bound on generalization performance. 
