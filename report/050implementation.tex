\section{Implementation} \label{implementation}

\subsection{Polynomial} 

At first, we tried to use \textit{vector} data structure from the \textit{std}
library to implement polynomial class since it can handle coefficients
dynamically. However due to the time consuming dynamic memory allocation process,
it has been changed to static process.  

We use template class with an integer template $n$ to represent the maximum
possible degree of polynomial. Which means although a polynomial class with
template $n$, it's highest degree could be $n1$ where $n1 \leq n$. Although in
this way, developers need to handle the possible change over the template when
writing some operators, like $*,+$, programs can avoid dynamic memory
allocation, which may save a lot of running time.

%\subsection{Replace $x$ with $x+a$}

%Changing $P(x)$ to $P(x+a)$ in a polynomial is a very common operation in both
%methods, since both methods need to use the coefficients of $P(x+a)$ to get the
%sign variance. 

%At first, we tried a very primitive idea for this process. Initialize a very
%simple polynomial $p = x+ a$. Then multiply it by coefficients and increase its
%order one by one. Add them together at last to get transformed polynomial.

%In order to optimize this process, we apply Taylor expansion. This process is
%also called Taylor shift\cite{Taylor}. It runs as appling Taylor expansion at
%point $a$ with $\Delta x = x$. Then we have:

%$$
%P(x+a) = P(a) + P'(a) *x + ... + \frac{1}{n!}P^{(n)}(a)*x^n
%$$

%This process avoids multiplication of two polynomials. Multiplication needs
%$O(n^2)$ operations in our implementation. In order to make sure Taylor
%expansion can improve the performance of the program, we made some experiments
%to compare the computation time of these two implementations. Results are in
%table~\ref{tb1}.  Coefficients of test polynomials are generated randomly from
%$-1000$ to $1000$. Since this project does not pay attention to polynomials with
%degrees higher than 6, we only test polynomials with degree from 3 to 6.

%\begin{center}
%\label{tb1}
%\begin{tabular}{ |c|c|c| } 
 %\hline

 %Degree  & Tylor Expansion   & Original Method\\ 

 %\hline
 %3   & 1876 $ns$  & 2272 $ns$\\ 
 %4   & 2640 $ns$  & 3439 $ns$\\ 
 %5   & 3307 $ns$  & 4910 $ns$\\ 
 %6   & 4574 $us$  & 7103 $us$\\ 
 %\hline
%\end{tabular}
%\end{center}

%As shown above, the running time of polynomial shifting with Taylor Expansion is
%about $25\%$  faster than the primitive one in our implementation.

\subsection{Approximate GCD}

Errors in floating point computation are unavoidable. Therefore, it is hard to
check if one polynomial is zero or not. Greatest common divisor and Square-free
decomposition can not perform perfectly all the time.

In order to solve this error problem, we tried tolerance. If a number
smaller than this tolerance, it will be treated as zero. At first we used fixed
tolerance, which cannot perform well as the magnitude of the largest coefficient
grows.

Then we tried the method introduced by Matu-Tarow\cite{Approximate}. It suggests
to turn polynomials \textit{regular} before performing $GCD$. A polynomial is
\textit{regular} means its leading coefficient is $O(1)$ and any other
coefficients either $O(1)$ or $0$. The notation $O(c)$ not same as the Landau’s
symbol and it means a number of approximately the same magnitude as $c$. Any
univariate polynomial can be transformed to \textit{regular} by scaling
$P\rightarrow \xi P $ and $x \rightarrow  \eta x$. 

However, it is hard to define the $\eta$ $\xi$ when transforming a
polynomial to \textit{regular}. Besides that, if we transform $x$ to $\eta x$,
roots will be changed accordingly. And the precision of the algorithm will be
changed. Furthermore, this process will introduce extra computation and make the
program less efficient.

\subsection{Interval arithmetic}

Due to the problems mentioned in the last part of the previous subsection, we discard
approximate GCD and use \textit{interval arithmetic}. Interval arithmetic represents each
value as a range of possibilities so that we can keep track of errors from floating
point computation in algorithms. We use interval arithmetic implemented in
\textit{boost} library.

For the numbers that can be represented by a computer perfectly, no error might be
introduced through computation. However, if we get some numbers that not
suitable for a computer to store, errors might appear. For example, $\frac{1}{3}$
might be represented as $0.33333333333 \pm \epsilon$, where $\epsilon$ might as small
as $1e^{-17}$.

%Another approach to handle instability of $GCD$ and Yun's algorithm is interval
%arithmetic, which was first introduced in numerical analysis by Moore in 1960’s.
%The main purpose of introducing interval computation is to increase the
%stability of floating point computation.

%With above problem, I discard the methods introduced by
%Matu-Tarow\cite{Approximate} and applied \textit{interval arithmetic} to the
%project. Every exact number in project has been changed to a interval that
%represents a set of possible values. If zero in such interval, this number will
%be seen as zero.

With interval arithmetic, the computation has been doubled since every
computation need to calculate both ends of a interval. However, since it can
handle the error from floating point computation, our program becomes more stable.

Although interval arithmetic can not make our program runs perfectly on all of
cases, using interval arithmetic allows us to observe how errors propagated
through computation. With this observation, we can figure out in what
condition our program might fail. This part will be discussed in
Section\ref{analysis}.

