\section{Resutl and analysis} \label{Analysis}

\subsection{Running Time}

Whit above implementation, I test the running time of these two methods. Result
can be found in following table \ref{tb2}. Same as mentioned of Tylor Expansion
test, test polynomials are generated with random coefficient from $-n$ to $n$ with
degree $n-1$. 

\begin{center}
\label{tb2}
\begin{tabular}{ |c|c|c| } 
 \hline

 $n$  & Budan's Theorem & Continued Fraction\\ 

 \hline
 10   & 8.430$ms$  & 0.786$ms$\\ 
 20   & 38.574$ms$  & 1.826$ms$\\ 
 50   & 208.707$ms$  & 3.387 $ms$\\ 
 100   & 988.346$ms$  & 11.301$ms$\\ 
 150   & 2574.148$ms$  & 12.656$ms$\\ 
 200   & 3918.112$ms$  & 21.634$ms$\\ 
 \hline
\end{tabular}
\end{center}

From the table~\ref{tb2}, we can see that Budan's Theorem method takes longer
time than Continued Fraction method. This might because with bisection method,
searching range need to be small enough before one can figure out if there is
root or not. However, above polynomials are generated with random coefficients,
which cannot guarantee it has real roots. 

In order to figure out the relationship between the running time with magnitude
of roots, I designed following experiments. Test polynomials has several roots
and have every roots has $p=0.5$ probability same as previous one. Then every
root is in ranger $[-max\_root, max\_root]$. Running time is shown in
table~\ref{tb3}. Since with large degree, some number might be truncated, I test
with polynomials with degree 5.

\begin{center}
\label{tb3}
\begin{tabular}{ |c|c|c| } 
 \hline

 $max\_root$  & Budan's Theorem & Continued Fraction\\ 

 \hline
 1   & 38$us$  & 38$us$\\ 
 20   & 31$us$  & 29$us$\\ 
 50   & 38$us$  & 39$us$\\ 
 100   & 42$us$  & 56$us$\\ 
 150   & 46$us$  & 70$us$\\ 
 200   & 46$us$  & 83$us$\\ 
 \hline
\end{tabular}
\end{center}

There are several reasons that makes running time in this table is much lesser
than the previous one. The first one is the degree of these polynomials are
small, only 5 actually. Second, there are repeat roots in these polynomials and
after square-free decomposition, square-free polynomials might only have small
degree like $1$ or $2$, which can be solved with closed formula solution.

And we can see that, running time of Continued Fraction method is longer than
Budan's Theorem. It's noticeable that $p'=p(x+1)$ line in Algorithm~\ref{alg4}.
It is only shift polynomial by $1$ for each time. Therefore, when root is large,
it takes longer for Continued Fraction method to shift to the place around
roots. This place is need to be optimized, if the shifting can be flexible and
adjustable according to the lower bound of roots of polynomial it would be
faster.

\subsection{Fail Conditions}

Although this implementation can work correctly in most cases, there are
some conditions that could lead to failure. 

Since both methods need to work on polynomial with no repeat roots, square-free
decomposition becomes the most important step in the program. However, with the
Yun's algorithm\cite{Yuns}, $GCD$ computation should be success and divisions
should be exact, which cannot be achieved by floating number. 

As mentioned before, introducing interval arithmetic can see how error
propagated along with square-free decomposition process. Although can not know
how exact errors changing over $GCD$ process, it's related with the magnitude of
the coefficients. 

The magnitude of coefficients influence the error through the process computing
the remainder of two polynomials. 

Let polynomial $P=a_0+a_1*x +
a_2*x^2...+a_n*x^n$, and its derivative
$P'=a_1+2*a_2*x...+n*a_n*x^{n-1}$. Let's simulate the first step of $GCD$
process, which is compute the $rem(P,P')$. All of coefficient in polynomial
$a_0...a_n$ are intervals with no error if it can represented by computer
perfectly

\begin{align*}
  div1 &= 1/n \\ 
  rem1 &= P - div * x * P' =  a_0 + \frac{n-1}{n}a_1*x+ \frac{n-2}{n}a_2*x^2....
\frac{1}{n}*a_{n-1}x^{n-1}\\ 
  div2 &= \frac{a_{n-1}}{a_n} * \frac{1}{n^2} \\ 
  rem2 &= rem1 - div2 * P' = (a_0 - div2*a_1) + (\frac{n-1}{n}*a_1 - div2 *
  2*a_2)+ ... \\
      &= \sum_{i=0}^{n-1} (\frac{n-i}{n}*a_i -
      \frac{a_{n-1}*a_{i+1}}{n^2*a_n}(i+1)) x^i
\end{align*} 

$rem2$ in above equations only has degree $n-1$ which is one less than $P'$,
therefore, it should be the remainder of $P/P'$. It's noticeable that $div2 =
\frac{a_{n-1}}{a_n} * \frac{1}{n^2}$ might be very large. Although a interval $a_i
= [a_i-\epsilon, a_i + \epsilon]$ might be small at first, when it multiplied by $div2$,
the interval 
will be enlarged to $a_i * div2 = [(a_i-\epsilon)*div2, (a_i+\epsilon)*div2]$
and might not be ignorable. Besides that, with the intervals enlarged, some
number might be regarded as zero accidentally. 

Therefore, if magnitude of coefficients is very large and roots are close enough, 
there are potential for this program to failed in $GCD$ process and lead to failure
in real-root isolation.



