
\section{Solution}
Since previous work in BGP anomaly detection has not thus far examined the generalization performance between different anomalies, we begin by surveying the generalizability of different anomaly detection methods on our three separate worm datasets. Using the information gleaned from this survey, we select the method with the highest generalizability (which we define as the highest mean F1 score), and perform a feature ablation experiment with it in order to ascertain which networking features are most important for generalizing between different the three different worms. 


\subsection{Generalization Performance}
In order to evaluate the inter-anomaly generalization performance of each method, we use three combinations of training and testing data where each method is trained on one of the anomalies and tested on the other two. 
% \begin{center}
% \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
%     \hline
%     \textbf{Method} & \textbf{C Acc} & \textbf{C F1} & \textbf{N Acc} & \textbf{N F1} & \textbf{S Acc} & \textbf{S F1} & \textbf{Mean Acc} & \textbf{Mean F1}\\
%     \hline
%     Jiaojiao Cheng & 70\%/50\% & 25\% & 25\% & 25\% & 95\% \\
%     Colin Lee & \% & 25\% & 25\% & 25\% & 95\%\\
%     Zhuocheng Shang & 85\%/50\% & 25\% & 25\% & 25\% & 90\%\\
%     William Shiao & 90\%/50\% & 25\% & 25\% & 25\% & 90\% \\
%     \hline
% \end{tabular}
% \end{center}

\begin{table} [ht!]
\centering
% \begin{ssmall}
\begin{tabular}{lll|ll|ll|ll}
\toprule
    & \multicolumn{2}{c|}{\textbf{Nimda}} & \multicolumn{2}{c|}{\textbf{Code Red}} & \multicolumn{2}{c|}{\textbf{Slammer}} & \multicolumn{2}{c}{\textbf{Mean}} \\
    \midrule
    \multicolumn{1}{c}{Model} & \multicolumn{1}{c}{Acc.} & \multicolumn{1}{c|}{F1} & \multicolumn{1}{c}{Acc.} & \multicolumn{1}{c|}{F1} & \multicolumn{1}{c}{Acc.} & \multicolumn{1}{c|}{F1} & \multicolumn{1}{c}{Acc.} & \multicolumn{1}{c}{F1} \\
    \bottomrule
    \toprule
    \multicolumn{9}{c}{\textbf{One-class Methods}} \\
    \midrule
    OC-SVM &0.102  &0.185  &0.253 &0.404  &0.237  &0.384  &0.198 & 0.324 \\
    Entropy OC-SVM &0.102  &0.185  &0.253 &0.404  &0.237  &0.384  &0.198 & 0.324\\
    Autoencoder & 0.903 & 0.501 & 0.608 & 0.282 & 0.745 & 0.530 & 0.752 & \textbf{0.438} \\
    Deep SVDD & 0.921 & 0.479 & 0.804 & 0.354 & 0.726 & 0.388 & \textbf{0.817} & 0.407 \\
    \bottomrule
    \toprule
    \multicolumn{9}{c}{\textbf{Unsupervised Methods}} \\
    \midrule
    KNN & 0.895 & 0.488 & 0.786 & 0.453 & 0.778 & 0.449 & \textbf{0.820} & \textbf{0.463}\\
    Isolation Forest  & 0.856 & 0.395 & 0.769 & 0.355 & 0.769 & 0.324 & 0.798 & 0.358 \\
    PCA-based & 0.827 & 0.295 & 0.711 & 0.245 & 0.747 & 0.177 & 0.762 & 0.239 \\
    LOF w/ Feature Bagging & 0.779 & 0.249 & 0.679 & 0.178 & 0.695 & 0.176 & 0.717 & 0.201 \\
    Angle-based Outlier Detector& 0.892 & 0.488 & 0.768 & 0.372 & 0.772 & 0.382 & 0.811 & 0.414\\
    \bottomrule
\end{tabular}
% \end{ssmall}
\caption{\label{tab:results} All of the scores shown are the scores when the model is trained on the listed dataset and evaluated on the remaining two datasets. LOF stands for Local Outlier Factor.}
\end{table}

We note that autoencoders appear to be the best one-class method and KNN is the best unsupervised method for generalizing between these datasets. In general, the unsupervised methods outperform the one-class methods, which is an expected result, given that the unsupervised methods learn from both anomalous and unanomalous data, whereas the one-class methods are limited to learning their decision boundaries from the unanomalous data. 


\subsection{Feature Ablation Analysis}
In order to explore \textit{what} makes these models generalizable, we ablatively remove features from our data and note which features are most important for the best methods. We then note the most and least important features based on how much they decrease the F1 score of the methods. 

\begin{table} [ht!]
\centering
\begin{tabular}{c|c|c}
\toprule
     \textbf{Method} & \textbf{Most important} & \textbf{Least important} \\
     \hline
     KNN & Number of withdrawn NRLI prefixes & Packet size \\
     & Number of announcements  & Number of duplicate withdrawals \\
     \hline
     ABOD & Number of withdrawn NRLI prefixes & Packet size\\
     & Avg unique AS-path & Number of duplicate withdrawals \\
     \hline
     AE & Avg AS-path length & Max AS-path length = 8 \\
     & Max AS-path length & Number of duplicate withdrawals \\
\bottomrule
\end{tabular}
\caption{\label{tab:feats} Top two most important and least important features for the three best methods: K-nearest neighbor, Angle-based Outlier Detector, Autoencoder}
\end{table}

In general, it seems that the number of withdrawn NLRI prefixes and one of the AS-path length features are important to the generalization performance of the each of the models. This suggests that generalizable models must key in on features that are informative about the reachability of other ASs. Worms such as the ones investigated in these datasets typically affect networks most when attempt to rapidly replicate themselves, overloading the capacity of the network and leading to Denial of Service events. As such, it would follow that some nodes become unreachable and different AS-paths must be found. 

Least important features included packet size, and the number of duplicate withdrawals. Packet size is negligible as packet size can vary normally for any number of reasons. That the number of duplicate withdrawals is unimportant to all three suggests that normal traffic sees a similar number of duplicate withdrawals as anomalous traffic.

\section{Challenges}

%We had trouble finding recent datasets for BGP anomaly detection, so we had to use these three datasets because they were one of the few available 

Since nobody else has performed this type of analysis on BGP data before, we also had difficulty deciding how exactly to evaluate our methods. We had a hard time deciding on how to evaluate the generalizabilty of our methods.

Ideally, the anomalies in our dataset would have represented a diverse array of anomalies from different time periods, but almost every dataset we encountered used a different set of features, which would have made comparison and analysis impossible with our current experimental framework.

We had to write code to load the datasets properly because of how we chose to evaluate the data. We also to re-implement several of methods because there were no implementations online that worked with our data. Examples of these were the Fence-GAN \cite{Ngo2019FenceDetection}, which had old code online, but it only worked with images and no longer ran on newer hardware. We reimplemented this from scratch only to find that it would take too long to train to a reasonable level of accuracy (which is why it is not included in the table). The autoencoder method was another example of this, where the papers describing it for anomaly detection lacked detail and a sample implementation, so we had to guess about how to fill in some of the blanks. Another challenge is the low accuracy and F1-score performance by the OC-SVM and entropy OCSVM, it lead our team to consider which entropy or one class method would fit better on such type of BGP dataset.


